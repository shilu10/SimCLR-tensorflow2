{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/mf1024/ImageNet-Datasets-Downloader.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport requests\nimport argparse\nimport json\nimport time\nimport logging\nimport csv\n\nfrom multiprocessing import Pool, Process, Value, Lock\nfrom requests.exceptions import ConnectionError, ReadTimeout, TooManyRedirects, MissingSchema, InvalidURL\n\ndebug = False\nimages_per_class = 500\nuse_class_list = True\nclass_list = [\"n02958343\",  'n02503517', 'n01323068', \"n02129165\", \n                                          \"n02691156\", \"n03793489\"]\nscrape_only_flickr = True\nnumber_of_classes = 10\nmultiprocessing_workers = 8\n\nIMAGENET_API_WNID_TO_URLS = lambda wnid: f'http://www.image-net.org/api/imagenet.synset.geturls?wnid={wnid}'\n\ncurrent_folder = 'ImageNet-Datasets-Downloader/'\n\nclass_info_json_filename = 'imagenet_class_info.json'\nclass_info_json_filepath = os.path.join(current_folder, class_info_json_filename)\n\nclass_info_dict = dict()\n\nwith open(class_info_json_filepath) as class_info_json_f:\n    class_info_dict = json.load(class_info_json_f)\n\nclasses_to_scrape = []\n\nif use_class_list == True:\n   for item in class_list:\n       classes_to_scrape.append(item)\n       if item not in class_info_dict:\n           logging.error(f'Class {item} not found in ImageNete')\n           exit()\n\nelif use_class_list == False:\n    potential_class_pool = []\n    for key, val in class_info_dict.items():\n\n        if scrape_only_flickr:\n            if int(val['flickr_img_url_count']) * 0.9 > images_per_class:\n                potential_class_pool.append(key)\n        else:\n            if int(val['img_url_count']) * 0.8 > images_per_class:\n                potential_class_pool.append(key)\n\n    if (len(potential_class_pool) < number_of_classes):\n        logging.error(f\"With {images_per_class} images per class there are {len(potential_class_pool)} to choose from.\")\n        logging.error(f\"Decrease number of classes or decrease images per class.\")\n        exit()\n\n    picked_classes_idxes = np.random.choice(len(potential_class_pool), number_of_classes, replace = False)\n\n    for idx in picked_classes_idxes:\n        classes_to_scrape.append(potential_class_pool[idx])\n\n\nprint(\"Picked the following clases:\")\nprint([ class_info_dict[class_wnid]['class_name'] for class_wnid in classes_to_scrape ])\n\nimagenet_images_folder = os.path.join(\"simclr_data\", 'imagenet_images')\nif not os.path.isdir(imagenet_images_folder):\n    os.mkdir(imagenet_images_folder)\n\n\nscraping_stats = dict(\n    all=dict(\n        tried=0,\n        success=0,\n        time_spent=0,\n    ),\n    is_flickr=dict(\n        tried=0,\n        success=0,\n        time_spent=0,\n    ),\n    not_flickr=dict(\n        tried=0,\n        success=0,\n        time_spent=0,\n    )\n)\n\ndef add_debug_csv_row(row):\n    with open('stats.csv', \"a\") as csv_f:\n        csv_writer = csv.writer(csv_f, delimiter=\",\")\n        csv_writer.writerow(row)\n\nclass MultiStats():\n    def __init__(self):\n\n        self.lock = Lock()\n\n        self.stats = dict(\n            all=dict(\n                tried=Value('d', 0),\n                success=Value('d',0),\n                time_spent=Value('d',0),\n            ),\n            is_flickr=dict(\n                tried=Value('d', 0),\n                success=Value('d',0),\n                time_spent=Value('d',0),\n            ),\n            not_flickr=dict(\n                tried=Value('d', 0),\n                success=Value('d', 0),\n                time_spent=Value('d', 0),\n            )\n        )\n    def inc(self, cls, stat, val):\n        with self.lock:\n            self.stats[cls][stat].value += val\n\n    def get(self, cls, stat):\n        with self.lock:\n            ret = self.stats[cls][stat].value\n        return ret\n\nmulti_stats = MultiStats()\n\n\nif False:\n    row = [\n        \"all_tried\",\n        \"all_success\",\n        \"all_time_spent\",\n        \"is_flickr_tried\",\n        \"is_flickr_success\",\n        \"is_flickr_time_spent\",\n        \"not_flickr_tried\",\n        \"not_flickr_success\",\n        \"not_flickr_time_spent\"\n    ]\n    add_debug_csv_row(row)\n\ndef add_stats_to_debug_csv():\n    row = [\n        multi_stats.get('all', 'tried'),\n        multi_stats.get('all', 'success'),\n        multi_stats.get('all', 'time_spent'),\n        multi_stats.get('is_flickr', 'tried'),\n        multi_stats.get('is_flickr', 'success'),\n        multi_stats.get('is_flickr', 'time_spent'),\n        multi_stats.get('not_flickr', 'tried'),\n        multi_stats.get('not_flickr', 'success'),\n        multi_stats.get('not_flickr', 'time_spent'),\n    ]\n    add_debug_csv_row(row)\n\ndef print_stats(cls, print_func):\n\n    actual_all_time_spent = time.time() - scraping_t_start.value\n    processes_all_time_spent = multi_stats.get('all', 'time_spent')\n\n    if processes_all_time_spent == 0:\n        actual_processes_ratio = 1.0\n    else:\n        actual_processes_ratio = actual_all_time_spent / processes_all_time_spent\n\n    #print(f\"actual all time: {actual_all_time_spent} proc all time {processes_all_time_spent}\")\n\n    print_func(f'STATS For class {cls}:')\n    print_func(f' tried {multi_stats.get(cls, \"tried\")} urls with'\n               f' {multi_stats.get(cls, \"success\")} successes')\n\n    if multi_stats.get(cls, \"tried\") > 0:\n        print_func(f'{100.0 * multi_stats.get(cls, \"success\")/multi_stats.get(cls, \"tried\")}% success rate for {cls} urls ')\n    if multi_stats.get(cls, \"success\") > 0:\n        print_func(f'{multi_stats.get(cls,\"time_spent\") * actual_processes_ratio / multi_stats.get(cls,\"success\")} seconds spent per {cls} succesful image download')\n\n\n\nlock = Lock()\nurl_tries = Value('d', 0)\nscraping_t_start = Value('d', time.time())\nclass_folder = ''\nclass_images = Value('d', 0)\n\ndef get_image(img_url):\n\n    #print(f'Processing {img_url}')\n\n    #time.sleep(3)\n\n    if len(img_url) <= 1:\n        return\n\n\n    cls_imgs = 0\n    with lock:\n        cls_imgs = class_images.value\n\n    if cls_imgs >= images_per_class:\n        return\n\n    logging.debug(img_url)\n\n    cls = ''\n\n    if 'flickr' in img_url:\n        cls = 'is_flickr'\n    else:\n        cls = 'not_flickr'\n        if scrape_only_flickr:\n            return\n\n    t_start = time.time()\n\n    def finish(status):\n        t_spent = time.time() - t_start\n        multi_stats.inc(cls, 'time_spent', t_spent)\n        multi_stats.inc('all', 'time_spent', t_spent)\n\n        multi_stats.inc(cls,'tried', 1)\n        multi_stats.inc('all', 'tried', 1)\n\n        if status == 'success':\n            multi_stats.inc(cls,'success', 1)\n            multi_stats.inc('all', 'success', 1)\n\n        elif status == 'failure':\n            pass\n        else:\n            logging.error(f'No such status {status}!!')\n            exit()\n        return\n\n\n    with lock:\n        url_tries.value += 1\n        if url_tries.value % 250 == 0:\n            print(f'\\nScraping stats:')\n            print_stats('is_flickr', print)\n            print_stats('not_flickr', print)\n            print_stats('all', print)\n            if debug:\n                add_stats_to_debug_csv()\n\n    try:\n        img_resp = requests.get(img_url, timeout = 1)\n    except ConnectionError:\n        logging.debug(f\"Connection Error for url {img_url}\")\n        return finish('failure')\n    except ReadTimeout:\n        logging.debug(f\"Read Timeout for url {img_url}\")\n        return finish('failure')\n    except TooManyRedirects:\n        logging.debug(f\"Too many redirects {img_url}\")\n        return finish('failure')\n    except MissingSchema:\n        return finish('failure')\n    except InvalidURL:\n        return finish('failure')\n\n    if not 'content-type' in img_resp.headers:\n        return finish('failure')\n\n    if not 'image' in img_resp.headers['content-type']:\n        logging.debug(\"Not an image\")\n        return finish('failure')\n\n    if (len(img_resp.content) < 1000):\n        return finish('failure')\n\n    logging.debug(img_resp.headers['content-type'])\n    logging.debug(f'image size {len(img_resp.content)}')\n\n    img_name = img_url.split('/')[-1]\n    img_name = img_name.split(\"?\")[0]\n\n    if (len(img_name) <= 1):\n        return finish('failure')\n\n    img_file_path = os.path.join(class_folder, img_name)\n    logging.debug(f'Saving image in {img_file_path}')\n\n    with open(img_file_path, 'wb') as img_f:\n        img_f.write(img_resp.content)\n\n        with lock:\n            class_images.value += 1\n\n        logging.debug(f'Scraping stats')\n        print_stats('is_flickr', logging.debug)\n        print_stats('not_flickr', logging.debug)\n        print_stats('all', logging.debug)\n\n        return finish('success')\n\n\nfor class_wnid in classes_to_scrape:\n\n    class_name = class_info_dict[class_wnid][\"class_name\"]\n    print(f'Scraping images for class \\\"{class_name}\\\"')\n    url_urls = IMAGENET_API_WNID_TO_URLS(class_wnid)\n\n    time.sleep(0.05)\n    resp = requests.get(url_urls)\n\n    class_folder = os.path.join(imagenet_images_folder, class_name)\n    if not os.path.exists(class_folder):\n        os.mkdir(class_folder)\n\n    class_images.value = 0\n\n    urls = [url.decode('utf-8') for url in resp.content.splitlines()]\n\n    #for url in  urls:\n    #    get_image(url)\n\n    print(f\"Multiprocessing workers: {multiprocessing_workers}\")\n    with Pool(processes=multiprocessing_workers) as p:\n        p.map(get_image,urls)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os \n\ndata_dir = \"simclr_data/imagenet_images/\"\n\nfor direc in os.listdir(data_dir): \n    \n    for indx, file in enumerate(os.listdir(data_dir+direc)): \n        src_filename = data_dir + direc + \"/\" + file\n        dst_filename = data_dir + direc + \"/\" + str(direc) + f\"_{indx}.jpg\"\n\n        if indx > 400: \n            os.remove(src_filename) \n\n        if str(direc)[0] != file[0] and indx <= 400: \n            os.rename(src_filename, dst_filename)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-24T01:53:15.535582Z","iopub.execute_input":"2023-04-24T01:53:15.536681Z","iopub.status.idle":"2023-04-24T01:53:15.611295Z","shell.execute_reply.started":"2023-04-24T01:53:15.536636Z","shell.execute_reply":"2023-04-24T01:53:15.610302Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"import shutil \nimport random\nimport numpy as np \n\ndef create_train_test_data(data_path, num_train, num_test): \n    directories = os.listdir(data_path)\n    if not os.path.exists(\"train\"):\n        os.mkdir(\"train\")\n        \n    if not os.path.exists(\"test\"):\n        os.mkdir(\"test\")\n    \n    for directory in directories: \n        try: \n            files_list = os.listdir(data_path+directory)\n            train_files = np.random.choice(files_list, num_train, replace=False)\n            test_files = [file for file in files_list if not file in train_files][: 100]\n\n            for train_file in train_files:\n                src = data_path + directory + '/' + train_file\n                dst = \"train\" + \"/\" + train_file\n                shutil.copyfile(src, dst)\n\n            for test_file in test_files:\n                src = data_path + directory + '/' + test_file\n                dst = \"test\" + \"/\" + test_file\n                shutil.copyfile(src, dst)\n                \n        except Exception as error:\n            return error\n            ","metadata":{"execution":{"iopub.status.busy":"2023-04-24T02:25:02.616573Z","iopub.execute_input":"2023-04-24T02:25:02.617713Z","iopub.status.idle":"2023-04-24T02:25:02.629323Z","shell.execute_reply.started":"2023-04-24T02:25:02.617668Z","shell.execute_reply":"2023-04-24T02:25:02.628284Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}