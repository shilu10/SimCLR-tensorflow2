{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!git clone https://github.com/mf1024/ImageNet-Datasets-Downloader.git\n","!pip install imutils"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import imutils \n","import os \n","import shutil\n","import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt "]},{"cell_type":"code","execution_count":216,"metadata":{"execution":{"iopub.execute_input":"2023-04-24T03:08:35.846415Z","iopub.status.busy":"2023-04-24T03:08:35.846000Z","iopub.status.idle":"2023-04-24T03:10:32.565816Z","shell.execute_reply":"2023-04-24T03:10:32.564450Z","shell.execute_reply.started":"2023-04-24T03:08:35.846375Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Picked the following clases:\n","['car', 'elephant', 'tiger cub', 'lion', 'airplane', 'mouse']\n","Scraping images for class \"car\"\n","Multiprocessing workers: 8\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 242.0 urls with 202.0 successes\n","83.47107438016529% success rate for is_flickr urls \n","0.02288476311334289 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 242.0 urls with 202.0 successes\n","83.47107438016529% success rate for all urls \n","0.023212544988877704 seconds spent per all succesful image download\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 492.0 urls with 407.0 successes\n","82.72357723577235% success rate for is_flickr urls \n","0.020271365382428203 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 494.0 urls with 407.0 successes\n","82.38866396761134% success rate for all urls \n","0.020354603080843237 seconds spent per all succesful image download\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 742.0 urls with 616.0 successes\n","82.90713324360699% success rate for is_flickr urls \n","0.022595397938360132 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 743.0 urls with 616.0 successes\n","82.90713324360699% success rate for all urls \n","0.022610659723158004 seconds spent per all succesful image download\n","Scraping images for class \"elephant\"\n","Multiprocessing workers: 8\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 992.0 urls with 810.0 successes\n","81.65322580645162% success rate for is_flickr urls \n","0.027982026559335214 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 993.0 urls with 810.0 successes\n","81.57099697885197% success rate for all urls \n","0.028047484233055586 seconds spent per all succesful image download\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 1243.0 urls with 1002.0 successes\n","80.61142397425583% success rate for is_flickr urls \n","0.0261859927114018 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 1243.0 urls with 1002.0 successes\n","80.61142397425583% success rate for all urls \n","0.026242201675673923 seconds spent per all succesful image download\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 1494.0 urls with 1200.0 successes\n","80.32128514056225% success rate for is_flickr urls \n","0.02491276418498383 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 1496.0 urls with 1200.0 successes\n","80.21390374331551% success rate for all urls \n","0.024941838184992474 seconds spent per all succesful image download\n","Scraping images for class \"tiger cub\"\n","Multiprocessing workers: 8\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 1742.0 urls with 1394.0 successes\n","80.02296211251435% success rate for is_flickr urls \n","0.02686234218318288 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 1743.0 urls with 1394.0 successes\n","79.97705106138841% success rate for all urls \n","0.026898610540578835 seconds spent per all succesful image download\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 1993.0 urls with 1571.0 successes\n","78.82589061716006% success rate for is_flickr urls \n","0.02615854583339037 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 1994.0 urls with 1571.0 successes\n","78.7863590772317% success rate for all urls \n","0.026186282419644505 seconds spent per all succesful image download\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 2242.0 urls with 1765.0 successes\n","78.72435325602142% success rate for is_flickr urls \n","0.02571385899616706 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 2242.0 urls with 1765.0 successes\n","78.72435325602142% success rate for all urls \n","0.025734350836648483 seconds spent per all succesful image download\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 2492.0 urls with 1961.0 successes\n","78.66024869634978% success rate for is_flickr urls \n","0.027005092172076917 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 2493.0 urls with 1961.0 successes\n","78.66024869634978% success rate for all urls \n","0.027013953518709433 seconds spent per all succesful image download\n","Scraping images for class \"lion\"\n","Multiprocessing workers: 8\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 2742.0 urls with 2151.0 successes\n","78.44638949671773% success rate for is_flickr urls \n","0.028402629645687655 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 2743.0 urls with 2151.0 successes\n","78.41779074006563% success rate for all urls \n","0.028436409047125438 seconds spent per all succesful image download\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 2992.0 urls with 2336.0 successes\n","78.07486631016043% success rate for is_flickr urls \n","0.027824344190016184 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 2992.0 urls with 2336.0 successes\n","78.07486631016043% success rate for all urls \n","0.027847695432297172 seconds spent per all succesful image download\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 3242.0 urls with 2533.0 successes\n","78.13078346699568% success rate for is_flickr urls \n","0.02709523578005207 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 3244.0 urls with 2533.0 successes\n","78.0826140567201% success rate for all urls \n","0.0271170774521564 seconds spent per all succesful image download\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 3492.0 urls with 2721.0 successes\n","77.92096219931271% success rate for is_flickr urls \n","0.027929192910690334 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 3492.0 urls with 2721.0 successes\n","77.92096219931271% success rate for all urls \n","0.027945323885911005 seconds spent per all succesful image download\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 3743.0 urls with 2910.0 successes\n","77.74512423189955% success rate for is_flickr urls \n","0.028775566877778044 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n","\n"," tried 3743.0 urls with 2910.0 successes77.74512423189955% success rate for all urls \n","0.028788030843964148 seconds spent per all succesful image download\n","Scraping images for class \"airplane\"\n","Multiprocessing workers: 8\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 3992.0 urls with 3097.0 successes\n","77.58016032064128% success rate for is_flickr urls \n","0.03023352636842909 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 3993.0 urls with 3097.0 successes\n","77.52190237797247% success rate for all urls \n","0.030270980174570146 seconds spent per all succesful image download\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 4242.0 urls with 3278.0 successes\n","77.27487034417727% success rate for is_flickr urls \n","0.029651911393980778 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 4243.0 urls with 3278.0 successes\n","77.25665802498233% success rate for all urls \n","0.029674277222015762 seconds spent per all succesful image download\n","Scraping images for class \"mouse\"\n","Multiprocessing workers: 8\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 4494.0 urls with 3455.0 successes\n","76.88028482421005% success rate for is_flickr urls \n","0.02953463435777016 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 4494.0 urls with 3455.0 successes\n","76.86318131256952% success rate for all urls \n","0.02955921409509504 seconds spent per all succesful image download\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 4742.0 urls with 3639.0 successes\n","76.73977224799663% success rate for is_flickr urls \n","0.02902420401147317 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 4742.0 urls with 3639.0 successes\n","76.73977224799663% success rate for all urls \n","0.029038912387102572 seconds spent per all succesful image download\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 4993.0 urls with 3830.0 successes\n","76.70739034648508% success rate for is_flickr urls \n","0.028686022198231347 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 4993.0 urls with 3830.0 successes\n","76.70739034648508% success rate for all urls \n","0.028696677765709302 seconds spent per all succesful image download\n","\n","Scraping stats:\n","STATS For class is_flickr:\n"," tried 5248.0 urls with 4019.0 successes\n","76.58155487804878% success rate for is_flickr urls \n","0.028750520940146122 seconds spent per is_flickr succesful image download\n","STATS For class not_flickr:\n"," tried 0.0 urls with 0.0 successes\n","STATS For class all:\n"," tried 5248.0 urls with 4019.0 successes\n","76.58155487804878% success rate for all urls \n","0.02875700451361359 seconds spent per all succesful image download\n"]}],"source":["import os\n","import numpy as np\n","import requests\n","import argparse\n","import json\n","import time\n","import logging\n","import csv\n","\n","from multiprocessing import Pool, Process, Value, Lock\n","from requests.exceptions import ConnectionError, ReadTimeout, TooManyRedirects, MissingSchema, InvalidURL\n","\n","debug = False\n","images_per_class = 1000\n","use_class_list = True\n","class_list = [\"n02958343\",  'n02503517', 'n01323068', \"n02129165\", \n","                                          \"n02691156\", \"n03793489\"]\n","scrape_only_flickr = True\n","number_of_classes = 10\n","multiprocessing_workers = 8\n","\n","IMAGENET_API_WNID_TO_URLS = lambda wnid: f'http://www.image-net.org/api/imagenet.synset.geturls?wnid={wnid}'\n","\n","current_folder = 'ImageNet-Datasets-Downloader/'\n","\n","class_info_json_filename = 'imagenet_class_info.json'\n","class_info_json_filepath = os.path.join(current_folder, class_info_json_filename)\n","\n","class_info_dict = dict()\n","\n","with open(class_info_json_filepath) as class_info_json_f:\n","    class_info_dict = json.load(class_info_json_f)\n","\n","classes_to_scrape = []\n","\n","if use_class_list == True:\n","   for item in class_list:\n","       classes_to_scrape.append(item)\n","       if item not in class_info_dict:\n","           logging.error(f'Class {item} not found in ImageNete')\n","           exit()\n","\n","elif use_class_list == False:\n","    potential_class_pool = []\n","    for key, val in class_info_dict.items():\n","\n","        if scrape_only_flickr:\n","            if int(val['flickr_img_url_count']) * 0.9 > images_per_class:\n","                potential_class_pool.append(key)\n","        else:\n","            if int(val['img_url_count']) * 0.8 > images_per_class:\n","                potential_class_pool.append(key)\n","\n","    if (len(potential_class_pool) < number_of_classes):\n","        logging.error(f\"With {images_per_class} images per class there are {len(potential_class_pool)} to choose from.\")\n","        logging.error(f\"Decrease number of classes or decrease images per class.\")\n","        exit()\n","\n","    picked_classes_idxes = np.random.choice(len(potential_class_pool), number_of_classes, replace = False)\n","\n","    for idx in picked_classes_idxes:\n","        classes_to_scrape.append(potential_class_pool[idx])\n","\n","\n","print(\"Picked the following clases:\")\n","print([ class_info_dict[class_wnid]['class_name'] for class_wnid in classes_to_scrape ])\n","\n","imagenet_images_folder = os.path.join(\"simclr_data\", 'imagenet_images')\n","if not os.path.isdir(imagenet_images_folder):\n","    os.mkdir(imagenet_images_folder)\n","\n","\n","scraping_stats = dict(\n","    all=dict(\n","        tried=0,\n","        success=0,\n","        time_spent=0,\n","    ),\n","    is_flickr=dict(\n","        tried=0,\n","        success=0,\n","        time_spent=0,\n","    ),\n","    not_flickr=dict(\n","        tried=0,\n","        success=0,\n","        time_spent=0,\n","    )\n",")\n","\n","def add_debug_csv_row(row):\n","    with open('stats.csv', \"a\") as csv_f:\n","        csv_writer = csv.writer(csv_f, delimiter=\",\")\n","        csv_writer.writerow(row)\n","\n","class MultiStats():\n","    def __init__(self):\n","\n","        self.lock = Lock()\n","\n","        self.stats = dict(\n","            all=dict(\n","                tried=Value('d', 0),\n","                success=Value('d',0),\n","                time_spent=Value('d',0),\n","            ),\n","            is_flickr=dict(\n","                tried=Value('d', 0),\n","                success=Value('d',0),\n","                time_spent=Value('d',0),\n","            ),\n","            not_flickr=dict(\n","                tried=Value('d', 0),\n","                success=Value('d', 0),\n","                time_spent=Value('d', 0),\n","            )\n","        )\n","    def inc(self, cls, stat, val):\n","        with self.lock:\n","            self.stats[cls][stat].value += val\n","\n","    def get(self, cls, stat):\n","        with self.lock:\n","            ret = self.stats[cls][stat].value\n","        return ret\n","\n","multi_stats = MultiStats()\n","\n","\n","if False:\n","    row = [\n","        \"all_tried\",\n","        \"all_success\",\n","        \"all_time_spent\",\n","        \"is_flickr_tried\",\n","        \"is_flickr_success\",\n","        \"is_flickr_time_spent\",\n","        \"not_flickr_tried\",\n","        \"not_flickr_success\",\n","        \"not_flickr_time_spent\"\n","    ]\n","    add_debug_csv_row(row)\n","\n","def add_stats_to_debug_csv():\n","    row = [\n","        multi_stats.get('all', 'tried'),\n","        multi_stats.get('all', 'success'),\n","        multi_stats.get('all', 'time_spent'),\n","        multi_stats.get('is_flickr', 'tried'),\n","        multi_stats.get('is_flickr', 'success'),\n","        multi_stats.get('is_flickr', 'time_spent'),\n","        multi_stats.get('not_flickr', 'tried'),\n","        multi_stats.get('not_flickr', 'success'),\n","        multi_stats.get('not_flickr', 'time_spent'),\n","    ]\n","    add_debug_csv_row(row)\n","\n","def print_stats(cls, print_func):\n","\n","    actual_all_time_spent = time.time() - scraping_t_start.value\n","    processes_all_time_spent = multi_stats.get('all', 'time_spent')\n","\n","    if processes_all_time_spent == 0:\n","        actual_processes_ratio = 1.0\n","    else:\n","        actual_processes_ratio = actual_all_time_spent / processes_all_time_spent\n","\n","    #print(f\"actual all time: {actual_all_time_spent} proc all time {processes_all_time_spent}\")\n","\n","    print_func(f'STATS For class {cls}:')\n","    print_func(f' tried {multi_stats.get(cls, \"tried\")} urls with'\n","               f' {multi_stats.get(cls, \"success\")} successes')\n","\n","    if multi_stats.get(cls, \"tried\") > 0:\n","        print_func(f'{100.0 * multi_stats.get(cls, \"success\")/multi_stats.get(cls, \"tried\")}% success rate for {cls} urls ')\n","    if multi_stats.get(cls, \"success\") > 0:\n","        print_func(f'{multi_stats.get(cls,\"time_spent\") * actual_processes_ratio / multi_stats.get(cls,\"success\")} seconds spent per {cls} succesful image download')\n","\n","\n","\n","lock = Lock()\n","url_tries = Value('d', 0)\n","scraping_t_start = Value('d', time.time())\n","class_folder = ''\n","class_images = Value('d', 0)\n","\n","def get_image(img_url):\n","\n","    #print(f'Processing {img_url}')\n","\n","    #time.sleep(3)\n","\n","    if len(img_url) <= 1:\n","        return\n","\n","\n","    cls_imgs = 0\n","    with lock:\n","        cls_imgs = class_images.value\n","\n","    if cls_imgs >= images_per_class:\n","        return\n","\n","    logging.debug(img_url)\n","\n","    cls = ''\n","\n","    if 'flickr' in img_url:\n","        cls = 'is_flickr'\n","    else:\n","        cls = 'not_flickr'\n","        if scrape_only_flickr:\n","            return\n","\n","    t_start = time.time()\n","\n","    def finish(status):\n","        t_spent = time.time() - t_start\n","        multi_stats.inc(cls, 'time_spent', t_spent)\n","        multi_stats.inc('all', 'time_spent', t_spent)\n","\n","        multi_stats.inc(cls,'tried', 1)\n","        multi_stats.inc('all', 'tried', 1)\n","\n","        if status == 'success':\n","            multi_stats.inc(cls,'success', 1)\n","            multi_stats.inc('all', 'success', 1)\n","\n","        elif status == 'failure':\n","            pass\n","        else:\n","            logging.error(f'No such status {status}!!')\n","            exit()\n","        return\n","\n","\n","    with lock:\n","        url_tries.value += 1\n","        if url_tries.value % 250 == 0:\n","            print(f'\\nScraping stats:')\n","            print_stats('is_flickr', print)\n","            print_stats('not_flickr', print)\n","            print_stats('all', print)\n","            if debug:\n","                add_stats_to_debug_csv()\n","\n","    try:\n","        img_resp = requests.get(img_url, timeout = 1)\n","    except ConnectionError:\n","        logging.debug(f\"Connection Error for url {img_url}\")\n","        return finish('failure')\n","    except ReadTimeout:\n","        logging.debug(f\"Read Timeout for url {img_url}\")\n","        return finish('failure')\n","    except TooManyRedirects:\n","        logging.debug(f\"Too many redirects {img_url}\")\n","        return finish('failure')\n","    except MissingSchema:\n","        return finish('failure')\n","    except InvalidURL:\n","        return finish('failure')\n","\n","    if not 'content-type' in img_resp.headers:\n","        return finish('failure')\n","\n","    if not 'image' in img_resp.headers['content-type']:\n","        logging.debug(\"Not an image\")\n","        return finish('failure')\n","\n","    if (len(img_resp.content) < 1000):\n","        return finish('failure')\n","\n","    logging.debug(img_resp.headers['content-type'])\n","    logging.debug(f'image size {len(img_resp.content)}')\n","\n","    img_name = img_url.split('/')[-1]\n","    img_name = img_name.split(\"?\")[0]\n","\n","    if (len(img_name) <= 1):\n","        return finish('failure')\n","\n","    img_file_path = os.path.join(class_folder, img_name)\n","    logging.debug(f'Saving image in {img_file_path}')\n","\n","    with open(img_file_path, 'wb') as img_f:\n","        img_f.write(img_resp.content)\n","\n","        with lock:\n","            class_images.value += 1\n","\n","        logging.debug(f'Scraping stats')\n","        print_stats('is_flickr', logging.debug)\n","        print_stats('not_flickr', logging.debug)\n","        print_stats('all', logging.debug)\n","\n","        return finish('success')\n","\n","\n","for class_wnid in classes_to_scrape:\n","\n","    class_name = class_info_dict[class_wnid][\"class_name\"]\n","    print(f'Scraping images for class \\\"{class_name}\\\"')\n","    url_urls = IMAGENET_API_WNID_TO_URLS(class_wnid)\n","\n","    time.sleep(0.05)\n","    resp = requests.get(url_urls)\n","\n","    class_folder = os.path.join(imagenet_images_folder, class_name)\n","    if not os.path.exists(class_folder):\n","        os.mkdir(class_folder)\n","\n","    class_images.value = 0\n","\n","    urls = [url.decode('utf-8') for url in resp.content.splitlines()]\n","\n","    #for url in  urls:\n","    #    get_image(url)\n","\n","    print(f\"Multiprocessing workers: {multiprocessing_workers}\")\n","    with Pool(processes=multiprocessing_workers) as p:\n","        p.map(get_image,urls)\n"]},{"cell_type":"code","execution_count":217,"metadata":{"execution":{"iopub.execute_input":"2023-04-24T03:11:14.440741Z","iopub.status.busy":"2023-04-24T03:11:14.439709Z","iopub.status.idle":"2023-04-24T03:11:14.501143Z","shell.execute_reply":"2023-04-24T03:11:14.500087Z","shell.execute_reply.started":"2023-04-24T03:11:14.440694Z"},"trusted":true},"outputs":[],"source":["import os \n","\n","DATA_DIR = \"simclr_data/imagenet_images/\"\n","\n","def rename_files(data_dir):\n","    \"\"\"\n","        this function, will rename the files in the main data directort. the filename will be their category name.\n","        Params:\n","            data_dir(type: str): Path, where the simclr imagenet data is saved.\n","        Return(None)\n","    \"\"\"\n","    for direc in os.listdir(data_dir): \n","        try: \n","            for indx, file in enumerate(os.listdir(data_dir+direc)): \n","                try: \n","                    src_filename = data_dir + direc + \"/\" + file\n","                    dst_filename = data_dir + direc + \"/\" + str(direc) + f\"_{indx}.jpg\"\n","\n","                    if indx >= 900: \n","                        os.remove(src_filename) \n","\n","                    if str(direc)[0] != file[0] and indx <= 400: \n","                        os.rename(src_filename, dst_filename)\n","                        \n","                except Exception as error:\n","                    continue\n","\n","        except Exception as error:\n","            return error \n","                "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rename_files(DATA_DIR)"]},{"cell_type":"code","execution_count":267,"metadata":{"execution":{"iopub.execute_input":"2023-04-24T03:24:10.436116Z","iopub.status.busy":"2023-04-24T03:24:10.435089Z","iopub.status.idle":"2023-04-24T03:24:10.448147Z","shell.execute_reply":"2023-04-24T03:24:10.446943Z","shell.execute_reply.started":"2023-04-24T03:24:10.436066Z"},"trusted":true},"outputs":[],"source":["import shutil \n","import random\n","import numpy as np \n","\n","def create_train_test_data(data_path, num_train, num_test, num_val): \n","    \"\"\"\n","        this function, will create a train, test, val directory by random picking the data from simclr imagenet data directory.\n","        Params:\n","            data_path(type: str): Path, where the simclr imagenet data is saved, \n","                                  so the function can read and put it into new directory.\n","            num_train(dtype: Int): Number of training examples.\n","            num_test(dtype: Int): Number of testing examples.\n","            num_val(dtype: Int): Number of Validation examples.\n","        Return(None)\n","    \"\"\"\n","    directories = os.listdir(data_path)\n","    if not os.path.exists(\"train\"):\n","        os.mkdir(\"train\")\n","        \n","    if not os.path.exists(\"test\"):\n","        os.mkdir(\"test\")\n","        \n","    if not os.path.exists(\"val\"):\n","        os.mkdir(\"val\")\n","    \n","    for directory in directories: \n","        try: \n","            files_list = os.listdir(data_path+directory)\n","            train_files = np.random.choice(files_list, num_train, replace=False)\n","            test_files = [file for file in files_list if not file in train_files][: 50]\n","            val_files = [file for file in files_list if (not file in train_files and not file in test_files)][: 50]\n","            \n","            for train_file in train_files:\n","                src = data_path + directory + '/' + train_file\n","                dst = \"train\" + \"/\" + train_file\n","                shutil.copyfile(src, dst)\n","\n","            for test_file in test_files:\n","                src = data_path + directory + '/' + test_file\n","                dst = \"test\" + \"/\" + test_file\n","                shutil.copyfile(src, dst)\n","                \n","            for val_file in val_files:\n","                src = data_path + directory + '/' + val_file\n","                dst = \"val\" + \"/\" + val_file\n","                shutil.copyfile(src, dst)\n","                \n","        except Exception as error:\n","            return error\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# creating train test and val directory.\n","create_train_test_data(data_dir, 340, 50, 50)"]},{"cell_type":"code","execution_count":273,"metadata":{"execution":{"iopub.execute_input":"2023-04-24T03:24:37.114041Z","iopub.status.busy":"2023-04-24T03:24:37.113714Z","iopub.status.idle":"2023-04-24T03:24:37.121591Z","shell.execute_reply":"2023-04-24T03:24:37.120562Z","shell.execute_reply.started":"2023-04-24T03:24:37.114010Z"},"trusted":true},"outputs":[],"source":["# creating a list of paths of train data, test and val data\n","\n","train_image_files = imutils.paths('train')\n","test_image_files = imutils.paths('test')\n","val_image_file = imutils.paths('val')"]},{"cell_type":"code","execution_count":207,"metadata":{"execution":{"iopub.execute_input":"2023-04-24T03:06:00.157349Z","iopub.status.busy":"2023-04-24T03:06:00.156281Z","iopub.status.idle":"2023-04-24T03:06:00.165880Z","shell.execute_reply":"2023-04-24T03:06:00.164601Z","shell.execute_reply.started":"2023-04-24T03:06:00.157298Z"},"trusted":true},"outputs":[],"source":["from tensorflow import keras \n","import tensorflow as tf \n","\n","\n","# Image preprocessing utils\n","@tf.function\n","def parse_images(image_path):\n","    \"\"\"\n","        this function, will read the image file into tensorflow object and resize it using tf.image library.\n","        Params:\n","            image_path(type: str): Path, where the images are saved.\n","        Return(type: tf,Tensor)\n","            returns the image as tf.Tensor object.\n","    \"\"\"\n","    image_string = tf.io.read_file(image_path)\n","    image = tf.image.decode_jpeg(image_string, channels=3)\n","    image = tf.image.convert_image_dtype(image, tf.float32)\n","    image = tf.image.resize(image, size=[224, 224])\n","\n","    return image\n","\n","def create_tensorflow_dataset(data, batch_size):\n","    \"\"\"\n","        this function, will convert the array of data into tensorflow dataset object.\n","        Params:\n","            data(type: np.array): Data, that needed to be converted.\n","            batch_size(type: Int): Number of batch size.\n","        Return(type: tf,data.Data)\n","            returns tensorflow data.Data object\n","    \"\"\"\n","    tensorflow_data = tf.data.Dataset.from_tensor_slices(train_image_files)\n","    tensorflow_data = (\n","    tensorflow_data\n","        .map(parse_images, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","        .shuffle(1024)\n","        .batch(batch_size, drop_remainder=True)\n","        .prefetch(tf.data.experimental.AUTOTUNE)\n","    )\n","    return tensorflow_data        "]},{"cell_type":"code","execution_count":209,"metadata":{"execution":{"iopub.execute_input":"2023-04-24T03:06:10.167567Z","iopub.status.busy":"2023-04-24T03:06:10.166131Z","iopub.status.idle":"2023-04-24T03:06:10.187803Z","shell.execute_reply":"2023-04-24T03:06:10.186637Z","shell.execute_reply.started":"2023-04-24T03:06:10.167518Z"},"trusted":true},"outputs":[],"source":["# convert the images files into tensorflow dataset.\n","\n","train_ds = create_tensorflow_dataset(train_image_files, 32)\n","test_ds = create_tensorflow_dataset(test_image_files, 32)\n","val_ds = create_tensorflow_dataset(test_image_files, 32)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
